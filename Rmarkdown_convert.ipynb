{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e57bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"WQD7004_Group_Assignment.ipynb\", encoding = \"UTF-8\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a6f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the programming language info\n",
    "programming_language = data[\"metadata\"][\"kernelspec\"][\"display_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc64b06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R\n"
     ]
    }
   ],
   "source": [
    "print(programming_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "652839a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to get the necessary information for each cell:\n",
    "cells = [ [x[\"cell_type\"], x[\"source\"]] for x in data[\"cells\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab49612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['markdown', ['# Predicting Public Health Outcomes Based on Global Climate Events Using Machine Learning']], ['markdown', ['WQD7004\\n', 'Group 5\\n', '\\n', '```\\n', '1. Ridhwan Bin Razali S2178595\\n', '2. Nurin Miza Afiqah Binti Andrie Dazlee S2154141\\n', '3. Maisie Chiara Salsabila 25062171\\n', '4. Amir Arifudeen Bin Mohd Fadzir 24232491\\n', '```\\n']], ['markdown', ['# Introduction\\n', '\\n', 'Climate change represents one of the most pressing global challenges of our time, with far-reaching implications for public health systems worldwide. The intersection of environmental conditions and human health outcomes has become increasingly critical as extreme weather events, air quality degradation, and temperature anomalies intensify across regions. However, comprehensive datasets linking climate indicators to health metrics across diverse geographic and socioeconomic contexts remain limited.\\n', '\\n', 'To address this gap, a climate-health dataset has been generated tracking 25 countries across 8 regions from 2015 to 2025. This dataset integrates climate indicators (temperature, precipitation, extreme weather events), air quality measurements (PM2.5, AQI), health outcomes (respiratory diseases, cardiovascular mortality, vector-borne disease risks), and socioeconomic factors (healthcare access, GDP per capita, mental health indices). Comprising 30 key attributes and 13,723 weekly records, it provides researchers and data scientists with a robust foundation for exploring climate-health relationships, developing predictive models, and informing evidence-based policy decisions.\\n', '\\n', '# Research Objectives\\n', '\\n', 'The primary objectives of working with this climate-health dataset are:\\n', '\\n', '1. To explore patterns and correlations between climate events (temperature anomalies, extreme weather, air pollution) and public health outcomes across different geographic regions and income levels.\\n', '2. To derive actionable insights related to respiratory diseases, cardiovascular mortality, vector-borne disease risks, and the influence of socioeconomic factors on health resilience.\\n', '\\n', '# Research Questions\\n', '\\n', 'This study/project seeks to address the following key research questions:\\n', '\\n', '1. How do climate indicators (temperature, precipitation, air quality) vary across regions and income levels, and what temporal trends emerge from 2015-2025?\\n', '2. Which climate and socioeconomic factors are most predictive of adverse health outcomes (respiratory diseases, cardiovascular mortality, heat-related admissions)?\\n', '\\n', '# Dataset Description\\n', '\\n', 'The dataset comprehensively tracks climate-health relationships across 25 countries from January 2015 to October 2025, providing weekly granularity for time-series analysis and regional comparisons.\\n', '\\n', '**Key Characteristics:**\\n', '\\n', '- **Source**: Dataset modeling real-world climate-health relationships\\n', '- **Dimension**: 30 columns Ã— 13,723 rows = 411,690 data points (after cleaning)\\n', '- **Geographic Coverage**: 25 countries across 8 regions (3 income levels)\\n', '- **Temporal Range**: January 2015 - October 2025 (weekly frequency, ~52-53 weeks/year)\\n', '- **Data Completeness**: 100% complete (0% missing values after validation)\\n', '\\n', '**Column Details:**\\n', '\\n', '**Geographic Dimensions (8 columns):**\\n', '- `record_id` (character): Unique identifier - e.g., \"REC-USA-2015-01\"\\n', '- `country_code` (character): ISO country code - e.g., \"USA\", \"CHN\", \"DEU\"\\n', '- `country_name` (character): Country name - e.g., \"United States\", \"China\", \"Germany\"\\n', '- `region` (character): Geographic region - e.g., \"North America\", \"East Asia\", \"Europe\"\\n', '- `income_level` (character): Economic classification - e.g., \"High\", \"Upper-Middle\", \"Lower-Middle\"\\n', '- `latitude` (numeric): Geographic latitude - e.g., 38.9072 (degrees)\\n', '- `longitude` (numeric): Geographic longitude - e.g., -77.0369 (degrees)\\n', '- `population_millions` (numeric): Population in millions - e.g., 331.9\\n', '\\n', '**Temporal Dimensions (4 columns):**\\n', '- `date` (Date): Weekly observation date - e.g., 2015-01-05\\n', '- `year` (numeric): Calendar year - e.g., 2015, 2020, 2025\\n', '- `month` (numeric): Month (1-12) - e.g., 1 (January), 12 (December)\\n', '- `week` (numeric): ISO week number (1-53) - e.g., 1, 26, 52\\n', '\\n', '**Climate Indicators (7 columns):**\\n', '- `temperature_celsius` (numeric): Average weekly temperature (Â°C) - e.g., 15.2, 28.7\\n', '- `temp_anomaly_celsius` (numeric): Temperature deviation from baseline (Â°C) - e.g., +1.3, -0.8\\n', '- `precipitation_mm` (numeric): Weekly precipitation (millimeters) - e.g., 25.4, 120.3\\n', '- `heat_wave_days` (numeric): Days with extreme heat (0-7) - e.g., 0, 3, 5\\n', '- `drought_indicator` (numeric): Drought severity index (0-3) - e.g., 0 (none), 2 (moderate)\\n', '- `flood_indicator` (numeric): Flood occurrence indicator (0-1) - e.g., 0 (no), 1 (yes)\\n', '- `extreme_weather_events` (numeric): Count of extreme events per week - e.g., 0, 2, 4\\n', '\\n', '**Air Quality (2 columns):**\\n', '- `pm25_ugm3` (numeric): PM2.5 concentration (Î¼g/mÂ³) - e.g., 12.5, 85.3\\n', '- `air_quality_index` (numeric): EPA Air Quality Index (0-500) - e.g., 42 (Good), 187 (Unhealthy)\\n', '\\n', '**Health Outcomes (5 columns):**\\n', '- `respiratory_disease_rate` (numeric): Cases per 100,000 population - e.g., 45.2, 128.7\\n', '- `cardio_mortality_rate` (numeric): Deaths per 100,000 population - e.g., 8.3, 23.1\\n', '- `vector_disease_risk_score` (numeric): Disease risk index (0-100) - e.g., 15.6, 67.4\\n', '- `waterborne_disease_incidents` (numeric): Weekly incident count - e.g., 2, 15, 37\\n', '- `heat_related_admissions` (numeric): Hospital admissions per week - e.g., 5, 42, 89\\n', '\\n', '**Socioeconomic & Wellbeing Indicators (4 columns):**\\n', '- `healthcare_access_index` (numeric): Population access percentage (0-100%) - e.g., 75.3, 95.8\\n', '- `gdp_per_capita_usd` (numeric): GDP per capita (USD) - e.g., 15,234, 65,789\\n', '- `mental_health_index` (numeric): Mental wellbeing score (0-100) - e.g., 68.2, 82.5\\n', '- `food_security_index` (numeric): Food security score (0-100) - e.g., 72.1, 91.3']], ['markdown', ['## Loading libraries']], ['code', ['# # Install required packages if not already installed\\n', '# if (!require(caret)) install.packages(\"caret\", repos = \"https://cran.rstudio.com/\")\\n', '# if (!require(dplyr)) install.packages(\"dplyr\", repos = \"https://cran.rstudio.com/\")\\n', '# if (!require(ggplot2)) install.packages(\"ggplot2\", repos = \"https://cran.rstudio.com/\")\\n', '# if (!require(MLmetrics)) install.packages(\"MLmetrics\", repos = \"https://cran.rstudio.com/\")\\n', '# if (!require(readr)) install.packages(\"readr\", repos = \"https://cran.rstudio.com/\")\\n', '# if (!require(repr)) install.packages(\"repr\", repos = \"https://cran.rstudio.com/\")\\n', '# if (!require(reshape2)) install.packages(\"reshape2\", repos = \"https://cran.rstudio.com/\")\\n', '# if (!require(ROSE)) install.packages(\"ROSE\", repos = \"https://cran.rstudio.com/\")\\n', '\\n', '# if (!require(lubridate)) install.packages(\"lubridate\", repos = \"https://cran.rstudio.com/\")\\n', '#  if (!require(e1071)) install.packages(\"e1071\", repos = \"https://cran.rstudio.com/\")']], ['code', ['# Load all required libraries\\n', 'library(caret)      # Classification and Regression Training\\n', 'library(dplyr)      # Data manipulation\\n', 'library(ggplot2)    # Data visualization\\n', 'library(MLmetrics)  # Machine Learning metrics\\n', 'library(readr)      # Fast data reading\\n', 'library(repr)       # Data representation\\n', 'library(reshape2)   # Data reshaping\\n', 'library(ROSE)       # Random Over-Sampling Examples\\n', 'library(lubridate)  # Date manipulation\\n', 'library(e1071)      # Statistical functions (skewness)\\n', '\\n', 'cat(\"All libraries loaded successfully! âœ“\")']], ['markdown', ['## Load the Dataset']], ['code', ['# Read the CSV of the dataset\\n', '# Google Drive URL converted to direct download format for Colab compatibility\\n', 'csv_url <- \"https://drive.google.com/uc?id=14kzQfJ7L_7NWBWbvX8RWBuQbAe93jPTy&export=download\"\\n', 'df <- read_csv(csv_url)\\n', '\\n', '# Store original dimensions before cleaning\\n', 'original_dim <- dim(df)']], ['code', ['str(df)']], ['code', ['# Check the shape of the dataset\\n', 'dim(df)']], ['markdown', ['* The dataset contain 14,100 rows and 30 columns\\n', '\\n']], ['code', ['sapply(df, function(x) sum(is.na(x)))']], ['markdown', ['* There is no missing values in the columns']], ['code', ['head(df)']], ['markdown', ['* There is no issue with inconsistent capitalization for the column names']], ['markdown', ['# Data Cleaning']], ['markdown', ['## Check for duplicate records']], ['markdown', ['Here we check the duplicates by looking at the record_id and the country + date.']], ['code', ['# Check duplicates by record_id (should be unique)\\n', 'dup_records <- sum(duplicated(df$record_id))\\n', '\\n', \"cat('Duplicate record_ids:', dup_records, '\\\\n')\\n\", 'total_duplicates_removed <- 0\\n', '\\n', '# Store total for summary\\n', '\\n', '# Check duplicates by country + date combination\\n', '\\n', 'if(\"country_name\" %in% colnames(df) && \"date\" %in% colnames(df)) {}\\n', '\\n', '  dup_country_date <- sum(duplicated(df[, c(\"country_name\", \"date\")]))\\n', \"  cat('Duplicate country+date combinations:', dup_country_date, '\\\\n')\"]], ['markdown', ['No duplicates are found']], ['markdown', ['## Validation - verify weekly records per country']], ['code', ['# Calculate records per country per year (should be approximately 52-53 weeks)\\n', 'if(\"country_name\" %in% colnames(df) && \"year\" %in% colnames(df)) {\\n', '  temporal_check <- df %>%\\n', '    group_by(country_name, year) %>%\\n', \"    summarise(record_count = n(), .groups = 'drop') %>%\\n\", '    filter(record_count < 45 | record_count > 55)\\n', '\\n', '  temporal_flags_count <- nrow(temporal_check)\\n', '\\n', '  if(temporal_flags_count > 0) {\\n', \"    cat('\\\\n=== ISSUES DETECTED ===\\\\n')\\n\", \"    cat('Country-year combinations with unusual record counts:\\\\n')\\n\", '    print(temporal_check, n = Inf)\\n', '  } else {\\n', \"    cat('Temporal validation passed - all country-years have ~52-53 records\\\\n')\\n\", '  }\\n', '\\n', '  temporal_issues <- temporal_check\\n', '} else {\\n', '  temporal_flags_count <- 0\\n', '  temporal_issues <- data.frame()\\n', \"  cat('Cannot perform temporal validation - required columns not found\\\\n')\\n\", '}']], ['markdown', ['A calendar year has approximately 52 weeks (365 days Ã· 7 days/week â‰ˆ 52.14 weeks). Some years have 53 weeks depending on how the year starts and ends.\\n', '\\n', '- However from the output above, there are some countries that have incomplete data for 2025\\n', \"- **Explanation**: Since we are currently in December 2025 and the year hasn't ended yet, the dataset contains approximately 38-42 weeks of data for 2025 across all countries\\n\", '- This is **expected behavior** and not a data quality issue\\n', '- **Recommendation for next step**: For year-over-year comparisons, consider:\\n', '  - Excluding 2025 data from annual analyses\\n', '  - Using only complete years (2015-2024) for trend analysis\\n', '  - Clearly documenting that 2025 represents partial year data when included in visualizations']], ['markdown', ['## Validate income level categories']], ['code', ['# Check income_level contains expected classifications\\n', 'expected_income_levels <- c(\"Low\", \"Lower-Middle\", \"Upper-Middle\", \"High\")\\n', '\\n', 'if(\"income_level\" %in% colnames(df)) {\\n', '  unique_income_levels <- unique(df$income_level)\\n', \"  cat('Income level categories found:\\\\n')\\n\", '  print(unique_income_levels)\\n', '\\n', '  # Check for unexpected categories\\n', '  unexpected_income_levels <- setdiff(unique_income_levels, expected_income_levels)\\n', '\\n', '  if(length(unexpected_income_levels) > 0) {\\n', \"    cat('\\\\nUnexpected income level categories detected:\\\\n')\\n\", '    print(unexpected_income_levels)\\n', '  } else {\\n', \"    cat('\\\\nAll income levels match expected classifications\\\\n')\\n\", '  }\\n', '} else {\\n', '  unexpected_income_levels <- c()\\n', \"  cat('income_level column not found\\\\n')\\n\", '}']], ['markdown', ['## Income Level Validation\\n', '\\n', 'The dataset contains 3 of the 4 expected income level classifications:\\n', '- **High**: High-income countries\\n', '- **Upper-Middle**: Upper-middle-income countries  \\n', '- **Lower-Middle**: Lower-middle-income countries\\n', '- **Low**: *(Not present in dataset)*\\n', '\\n', '**Note:** The \"Low\" income category is absent from the dataset, meaning no low-income countries are included in this climate-health study.\\n', 'All present categories follow the expected classification format.']], ['markdown', ['## Validate geographic coordinates']], ['code', ['# Check coordinate validity and consistency\\n', 'lat_violations <- 0\\n', 'lon_violations <- 0\\n', 'coord_missing <- 0\\n', 'coord_records_removed <- 0\\n', 'inconsistent_coord_countries <- c()\\n', '\\n', 'if(\"latitude\" %in% colnames(df) && \"longitude\" %in% colnames(df)) {\\n', '  # Check for values outside valid ranges\\n', '  lat_violations <- sum(df$latitude < -90 | df$latitude > 90, na.rm = TRUE)\\n', '  lon_violations <- sum(df$longitude < -180 | df$longitude > 180, na.rm = TRUE)\\n', '  coord_missing <- sum(is.na(df$latitude) | is.na(df$longitude))\\n', '\\n', \"  cat('=== GEOGRAPHIC COORDINATE VALIDATION ===\\\\n')\\n\", \"  cat('Latitude violations (outside -90 to 90):', lat_violations, '\\\\n')\\n\", \"  cat('Longitude violations (outside -180 to 180):', lon_violations, '\\\\n')\\n\", \"  cat('Missing coordinates:', coord_missing, '\\\\n')\\n\", '\\n', '  # Check coordinate consistency per country\\n', '  if(\"country_name\" %in% colnames(df)) {\\n', '    coord_consistency <- df %>%\\n', '      group_by(country_name) %>%\\n', '      summarise(\\n', '        unique_coords = n_distinct(paste(latitude, longitude)),\\n', \"        .groups = 'drop'\\n\", '      ) %>%\\n', '      filter(unique_coords > 1)\\n', '\\n', '    if(nrow(coord_consistency) > 0) {\\n', '      inconsistent_coord_countries <- coord_consistency$country_name\\n', \"      cat('\\\\nCountries with inconsistent coordinates:', length(inconsistent_coord_countries), '\\\\n')\\n\", \"      cat('Flagged countries:\\\\n')\\n\", '      print(inconsistent_coord_countries)\\n', '    } else {\\n', \"      cat('\\\\nAll countries have consistent coordinates\\\\n')\\n\", '    }\\n', '  }\\n', '\\n', '  # Remove records with clearly invalid coordinates\\n', '  if(lat_violations > 0 || lon_violations > 0) {\\n', '    original_rows <- nrow(df)\\n', '    df <- df %>%\\n', '      filter(latitude >= -90 & latitude <= 90 & longitude >= -180 & longitude <= 180)\\n', '    coord_records_removed <- original_rows - nrow(df)\\n', \"    cat('\\\\nRemoved', coord_records_removed, 'records with invalid coordinates\\\\n')\\n\", '  }\\n', '} else {\\n', \"  cat('Coordinate columns not found\\\\n')\\n\", '}']], ['markdown', ['## Validate climate and health data quality']], ['code', ['# Validate climate and health data for impossible values\\n', 'climate_quality_records_removed <- 0\\n', 'data_violations <- list()\\n', '\\n', \"cat('=== CLIMATE AND HEALTH DATA QUALITY VALIDATION ===\\\\n\\\\n')\\n\", '\\n', '# Temperature check (-50 to 50 Celsius is reasonable range)\\n', 'if(\"temperature_celsius\" %in% colnames(df)) {\\n', '  temp_violations <- sum(df$temperature_celsius < -50 | df$temperature_celsius > 50, na.rm = TRUE)\\n', '  data_violations$temperature <- temp_violations\\n', \"  cat('Temperature violations (outside -50 to 50Â°C):', temp_violations, '\\\\n')\\n\", '}\\n', '\\n', '# Check negative values in rate/count columns (should be non-negative)\\n', 'rate_columns <- c(\"respiratory_disease_rate\", \"cardio_mortality_rate\", \"vector_disease_risk_score\",\\n', '                  \"waterborne_disease_incidents\", \"heat_related_admissions\")\\n', 'for(col in rate_columns) {\\n', '  if(col %in% colnames(df)) {\\n', '    neg_count <- sum(df[[col]] < 0, na.rm = TRUE)\\n', '    if(neg_count > 0) {\\n', '      data_violations[[col]] <- neg_count\\n', \"      cat(col, 'negative values:', neg_count, '\\\\n')\\n\", '    }\\n', '  }\\n', '}\\n', '\\n', '# Check index bounds\\n', '# Healthcare Access Index: 0-100\\n', 'if(\"healthcare_access_index\" %in% colnames(df)) {\\n', '  hai_violations <- sum(df$healthcare_access_index < 0 | df$healthcare_access_index > 100, na.rm = TRUE)\\n', '  data_violations$healthcare_access_index <- hai_violations\\n', \"  cat('Healthcare Access Index violations (outside 0-100):', hai_violations, '\\\\n')\\n\", '\\n', '  # Show examples of violations\\n', '  if(hai_violations > 0) {\\n', '    hai_examples <- df %>%\\n', '      filter(healthcare_access_index < 0 | healthcare_access_index > 100) %>%\\n', '      select(record_id, country_name, date, healthcare_access_index) %>%\\n', '      head(5)\\n', \"    cat('  Example violations:\\\\n')\\n\", '    print(hai_examples)\\n', '  }\\n', '}\\n', '\\n', '# Air Quality Index: 0-500\\n', 'if(\"air_quality_index\" %in% colnames(df)) {\\n', '  aqi_violations <- sum(df$air_quality_index < 0 | df$air_quality_index > 500, na.rm = TRUE)\\n', '  data_violations$air_quality_index <- aqi_violations\\n', \"  cat('Air Quality Index violations (outside 0-500):', aqi_violations, '\\\\n')\\n\", '\\n', '  # Show examples of violations\\n', '  if(aqi_violations > 0) {\\n', '    aqi_examples <- df %>%\\n', '      filter(air_quality_index < 0 | air_quality_index > 500) %>%\\n', '      select(record_id, country_name, date, air_quality_index) %>%\\n', '      head(5)\\n', \"    cat('  Example violations:\\\\n')\\n\", '    print(aqi_examples)\\n', '  }\\n', '}\\n', '\\n', '# Mental Health Index: 0-100\\n', 'if(\"mental_health_index\" %in% colnames(df)) {\\n', '  mhi_violations <- sum(df$mental_health_index < 0 | df$mental_health_index > 100, na.rm = TRUE)\\n', '  data_violations$mental_health_index <- mhi_violations\\n', \"  cat('Mental Health Index violations (outside 0-100):', mhi_violations, '\\\\n')\\n\", '}\\n', '\\n', '# Food Security Index: 0-100\\n', 'if(\"food_security_index\" %in% colnames(df)) {\\n', '  fsi_violations <- sum(df$food_security_index < 0 | df$food_security_index > 100, na.rm = TRUE)\\n', '  data_violations$food_security_index <- fsi_violations\\n', \"  cat('Food Security Index violations (outside 0-100):', fsi_violations, '\\\\n')\\n\", '}\\n', '\\n', '# Population should be positive\\n', 'if(\"population_millions\" %in% colnames(df)) {\\n', '  pop_violations <- sum(df$population_millions < 0, na.rm = TRUE)\\n', '  data_violations$population_millions <- pop_violations\\n', \"  cat('Population violations (negative values):', pop_violations, '\\\\n')\\n\", '}\\n', '\\n', '# Remove records with impossible values\\n', 'original_rows <- nrow(df)\\n', 'df <- df %>%\\n', '  filter(\\n', '    if(\"temperature_celsius\" %in% colnames(.)) temperature_celsius >= -50 & temperature_celsius <= 50 else TRUE,\\n', '    if(\"respiratory_disease_rate\" %in% colnames(.)) respiratory_disease_rate >= 0 | is.na(respiratory_disease_rate) else TRUE,\\n', '    if(\"cardio_mortality_rate\" %in% colnames(.)) cardio_mortality_rate >= 0 | is.na(cardio_mortality_rate) else TRUE,\\n', '    if(\"vector_disease_risk_score\" %in% colnames(.)) vector_disease_risk_score >= 0 | is.na(vector_disease_risk_score) else TRUE,\\n', '    if(\"waterborne_disease_incidents\" %in% colnames(.)) waterborne_disease_incidents >= 0 | is.na(waterborne_disease_incidents) else TRUE,\\n', '    if(\"heat_related_admissions\" %in% colnames(.)) heat_related_admissions >= 0 | is.na(heat_related_admissions) else TRUE,\\n', '    if(\"healthcare_access_index\" %in% colnames(.)) healthcare_access_index >= 0 & healthcare_access_index <= 100 | is.na(healthcare_access_index) else TRUE,\\n', '    if(\"air_quality_index\" %in% colnames(.)) air_quality_index >= 0 & air_quality_index <= 500 | is.na(air_quality_index) else TRUE,\\n', '    if(\"mental_health_index\" %in% colnames(.)) mental_health_index >= 0 & mental_health_index <= 100 | is.na(mental_health_index) else TRUE,\\n', '    if(\"food_security_index\" %in% colnames(.)) food_security_index >= 0 & food_security_index <= 100 | is.na(food_security_index) else TRUE,\\n', '    if(\"population_millions\" %in% colnames(.)) population_millions >= 0 | is.na(population_millions) else TRUE\\n', '  )\\n', '\\n', 'climate_quality_records_removed <- original_rows - nrow(df)\\n', \"cat('\\\\nRemoved', climate_quality_records_removed, 'records with invalid climate/health data\\\\n')\"]], ['markdown', ['## Data Quality Validation Results\\n', '\\n', '### Issues Found & Resolved:\\n', '\\n', '**1. Healthcare Access Index (3 violations)**\\n', '- Valid range: 0-100% (percentage of population with healthcare access)\\n', '- Found: UK records with 102% (impossible)\\n', '- Action: Removed 3 records\\n', '\\n', '**2. Air Quality Index (374 violations)**\\n', '- Valid range: 0-500 (EPA scale)\\n', '- Found: Negative values (-3 to -25) in US data (2015-2016)\\n', '- Action: Removed 374 records\\n', '\\n', '**3. Other Metrics**\\n', '- Temperature, Mental Health Index, Food Security Index, Population: All valid\\n', '\\n', '### Summary:\\n', '- **377 total records removed** (~2.7% of dataset)\\n', '- Likely data entry/collection errors from 2015-2018\\n', '- Dataset now contains only physically possible values\\n', '\\n', '**Note:** Healthcare Access >100% and negative Air Quality Index values are logically impossible and indicate data quality issues requiring removal.']], ['markdown', ['## Reorder columns by category']], ['code', ['# Organize columns in logical order for analysis\\n', 'column_order <- c(\\n', '  # Geographic information (8)\\n', '  \"record_id\", \"country_code\", \"country_name\", \"region\",\\n', '  \"income_level\", \"latitude\", \"longitude\", \"population_millions\",\\n', '\\n', '  # Temporal information (4)\\n', '  \"date\", \"year\", \"month\", \"week\",\\n', '\\n', '  # Climate indicators (8)\\n', '  \"temperature_celsius\", \"temp_anomaly_celsius\", \"precipitation_mm\", \"heat_wave_days\",\\n', '  \"drought_indicator\", \"flood_indicator\", \"extreme_weather_events\",\\n', '\\n', '  # Air quality (2)\\n', '  \"pm25_ugm3\", \"air_quality_index\",\\n', '\\n', '  # Health outcomes (5)\\n', '  \"respiratory_disease_rate\", \"cardio_mortality_rate\", \"vector_disease_risk_score\",\\n', '  \"waterborne_disease_incidents\", \"heat_related_admissions\",\\n', '\\n', '  # Socioeconomic and wellbeing (4)\\n', '  \"healthcare_access_index\", \"gdp_per_capita_usd\", \"mental_health_index\", \"food_security_index\"\\n', ')\\n', '\\n', '# Reorder columns\\n', 'df <- df[, column_order]\\n', '\\n', \"cat('Columns reordered into logical categories:\\\\n')\\n\", \"cat('- Geographic (8 columns)\\\\n')\\n\", \"cat('- Temporal (4 columns)\\\\n')\\n\", \"cat('- Climate (8 columns)\\\\n')\\n\", \"cat('- Air Quality (2 columns)\\\\n')\\n\", \"cat('- Health Outcomes (5 columns)\\\\n')\\n\", \"cat('- Socioeconomic/Wellbeing (4 columns)\\\\n')\"]], ['markdown', ['## Cleaned Dataset Structure\\n', 'Split by category']], ['code', ['# Display final column information organized by category\\n', \"cat('=== CLEANED DATASET STRUCTURE ===\\\\n\\\\n')\\n\", '\\n', \"cat('GEOGRAPHIC INFORMATION (8 columns):\\\\n')\\n\", \"cat('  1. record_id:', class(df$record_id), '\\\\n')\\n\", \"cat('  2. country_code:', class(df$country_code), '\\\\n')\\n\", \"cat('  3. country_name:', class(df$country_name), '\\\\n')\\n\", \"cat('  4. region:', class(df$region), '\\\\n')\\n\", \"cat('  5. income_level:', class(df$income_level), '\\\\n')\\n\", \"cat('  6. latitude:', class(df$latitude), '\\\\n')\\n\", \"cat('  7. longitude:', class(df$longitude), '\\\\n')\\n\", \"cat('  8. population_millions:', class(df$population_millions), '\\\\n\\\\n')\\n\", '\\n', \"cat('TEMPORAL INFORMATION (4 columns):\\\\n')\\n\", \"cat('  9. date:', class(df$date), '\\\\n')\\n\", \"cat(' 10. year:', class(df$year), '\\\\n')\\n\", \"cat(' 11. month:', class(df$month), '\\\\n')\\n\", \"cat(' 12. week:', class(df$week), '\\\\n\\\\n')\\n\", '\\n', \"cat('CLIMATE INDICATORS (7 columns):\\\\n')\\n\", \"cat(' 13. temperature_celsius:', class(df$temperature_celsius), '\\\\n')\\n\", \"cat(' 14. temp_anomaly_celsius:', class(df$temp_anomaly_celsius), '\\\\n')\\n\", \"cat(' 15. precipitation_mm:', class(df$precipitation_mm), '\\\\n')\\n\", \"cat(' 16. heat_wave_days:', class(df$heat_wave_days), '\\\\n')\\n\", \"cat(' 17. drought_indicator:', class(df$drought_indicator), '\\\\n')\\n\", \"cat(' 18. flood_indicator:', class(df$flood_indicator), '\\\\n')\\n\", \"cat(' 19. extreme_weather_events:', class(df$extreme_weather_events), '\\\\n\\\\n')\\n\", '\\n', \"cat('AIR QUALITY (2 columns):\\\\n')\\n\", \"cat(' 20. pm25_ugm3:', class(df$pm25_ugm3), '\\\\n')\\n\", \"cat(' 21. air_quality_index:', class(df$air_quality_index), '\\\\n\\\\n')\\n\", '\\n', \"cat('HEALTH OUTCOMES (5 columns):\\\\n')\\n\", \"cat(' 22. respiratory_disease_rate:', class(df$respiratory_disease_rate), '\\\\n')\\n\", \"cat(' 23. cardio_mortality_rate:', class(df$cardio_mortality_rate), '\\\\n')\\n\", \"cat(' 24. vector_disease_risk_score:', class(df$vector_disease_risk_score), '\\\\n')\\n\", \"cat(' 25. waterborne_disease_incidents:', class(df$waterborne_disease_incidents), '\\\\n')\\n\", \"cat(' 26. heat_related_admissions:', class(df$heat_related_admissions), '\\\\n\\\\n')\\n\", '\\n', \"cat('SOCIOECONOMIC & WELLBEING (4 columns):\\\\n')\\n\", \"cat(' 27. healthcare_access_index:', class(df$healthcare_access_index), '\\\\n')\\n\", \"cat(' 28. gdp_per_capita_usd:', class(df$gdp_per_capita_usd), '\\\\n')\\n\", \"cat(' 29. mental_health_index:', class(df$mental_health_index), '\\\\n')\\n\", \"cat(' 30. food_security_index:', class(df$food_security_index), '\\\\n')\"]], ['markdown', ['## Summary']], ['code', ['# Document issues flagged during data cleaning for team review\\n', \"cat('=== ISSUES FLAGGED FOR REVIEW ===\\\\n\\\\n')\\n\", '\\n', '# Missing values check\\n', \"cat('1. MISSING VALUES:\\\\n')\\n\", 'missing_counts <- colSums(is.na(df))\\n', 'total_missing <- sum(missing_counts)\\n', 'if(total_missing > 0) {\\n', \"  cat('   Total missing values:', total_missing, '\\\\n')\\n\", '  cols_with_missing <- names(missing_counts[missing_counts > 0])\\n', '  for(col in cols_with_missing) {\\n', '    pct <- round(missing_counts[col] / nrow(df) * 100, 1)\\n', \"    cat('   -', col, ':', missing_counts[col], 'missing (', pct, '%)\\\\n')\\n\", '  }\\n', '} else {\\n', \"  cat('   No missing values in the dataset\\\\n')\\n\", '}\\n', \"cat('\\\\n')\\n\", '\\n', '# Coordinate inconsistencies\\n', \"cat('2. COORDINATE INCONSISTENCIES:\\\\n')\\n\", 'if(exists(\"inconsistent_coord_countries\") && length(inconsistent_coord_countries) > 0) {\\n', \"  cat('   Countries with varying coordinates:', length(inconsistent_coord_countries), '\\\\n')\\n\", \"  cat('   Flagged for manual review:\\\\n')\\n\", '  for(country in inconsistent_coord_countries) {\\n', \"    cat('   -', country, '\\\\n')\\n\", '  }\\n', '} else {\\n', \"  cat('   None found\\\\n')\\n\", '}\\n', \"cat('\\\\n')\\n\", '\\n', '# Temporal gaps\\n', \"cat('3. TEMPORAL CONSISTENCY:\\\\n')\\n\", 'if(exists(\"temporal_issues\") && nrow(temporal_issues) > 0) {\\n', \"  cat('   Countries with unexpected weekly record counts:\\\\n')\\n\", '  print(temporal_issues)\\n', '} else {\\n', \"  cat('   None found\\\\n')\\n\", '}\\n', \"cat('\\\\n')\\n\", '\\n', '# Week mismatches\\n', \"cat('4. WEEK NUMBER DISCREPANCIES:\\\\n')\\n\", 'if(exists(\"week_mismatches\") && week_mismatches > 0) {\\n', \"  cat('   Records where week column differs from calculated ISO week:', week_mismatches, '\\\\n')\\n\", \"  cat('   Consider reviewing week column calculation methodology\\\\n')\\n\", '} else {\\n', \"  cat('   None found\\\\n')\\n\", '}\\n', \"cat('\\\\n')\\n\", '\\n', '# Income level issues\\n', \"cat('5. INCOME LEVEL VALIDATION:\\\\n')\\n\", 'if(exists(\"unexpected_income_levels\") && length(unexpected_income_levels) > 0) {\\n', \"  cat('   Unexpected income level categories found:\\\\n')\\n\", '  for(level in unexpected_income_levels) {\\n', \"    cat('   -', level, '\\\\n')\\n\", '  }\\n', '} else {\\n', \"  cat('   All income levels validated successfully\\\\n')\\n\", '}\\n', \"cat('\\\\n')\\n\", '\\n', '# Data validation summary\\n', \"cat('6. DATA QUALITY ISSUES RESOLVED:\\\\n')\\n\", \"cat('   Duplicate records removed:', total_duplicates_removed, '\\\\n')\\n\", \"cat('   Invalid coordinates removed:', coord_records_removed, '\\\\n')\\n\", \"cat('   Invalid climate/health data removed:', climate_quality_records_removed, '\\\\n')\\n\", \"cat('\\\\n')\\n\"]], ['markdown', ['## To take note:\\n', '\\n', '### **âš ï¸ 2025 Data is Incomplete**:\\n', 'The dataset contains only 42 weeks of data for 2025 (partial year).\\n', '- **Recommendation**: Exclude 2025 from year-over-year trend analysis\\n', '- **Use only 2015-2024** (10 complete years) for annual comparisons\\n', '- If including 2025, clearly label it as \"partial year data\"\\n', '\\n', '### **Data Quality Cleaning**:\\n', '377 records were removed due to invalid climate/health values:\\n', '- Healthcare Access Index violations: 3 records (>100%)\\n', '- Air Quality Index violations: 374 records (>500)\\n', '- All other validation checks passed\\n', '\\n', '### **Categorical Variables**:\\n', 'Several columns need factor conversion for analysis:\\n', '- `country_code`, `country_name`, `region`\\n', '- `income_level` (ordered factor: Lower-Middle < Upper-Middle < High)\\n', '- Optional: `drought_indicator`, `flood_indicator`\\n', '\\n', '### **Income Level Note**:\\n', '\"Low\" income category is absent from the dataset. Only 3 categories present:\\n', '- High\\n', '- Upper-Middle  \\n', '- Lower-Middle']], ['markdown', ['## Preview of Cleaned Dataset']], ['code', ['# Display first few rows of cleaned dataset\\n', \"cat('First 10 rows of cleaned dataset:\\\\n\\\\n')\\n\", 'head(df, 10)']], ['markdown', ['# Data visualization/EDA']], ['code', ['# install.packages(\"corrplot\")\\n', '# install.packages(\"skimr\")\\n', '# Load libraries\\n', 'library(corrplot)\\n', 'library(skimr)\\n', 'library(ggplot2)\\n', 'library(tidyr)\\n', 'library(ggplot2)\\n', 'library(reshape2)']], ['markdown', ['## Dataset Overview\\n', '\\n', '- ~13,700 rows, 30 columns\\n', '- 25 countries, 8 regions, 3 income levels\\n', '- Variables: climate, health, socio-economic\\n', '\\n']], ['code', ['nrow(df); ncol(df)\\n', 'length(unique(df$country_code))\\n', 'length(unique(df$region))\\n', 'length(unique(df$income_level))\\n']], ['markdown', ['## Data Quality\\n', '- No major missing values\\n', '- Strong categorical diversity']], ['code', ['colSums(is.na(df))\\n', 'table(df$income_level)']], ['markdown', ['## Histograms']], ['code', ['# Select only numeric columns for histograms\\n', 'numeric_cols <- names(df)[sapply(df, is.numeric)]\\n', '\\n', '# Exclude identifier and temporal columns that are not suitable for histograms\\n', 'exclude_cols <- c(\"record_id\", \"year\", \"month\", \"week\", \"latitude\", \"longitude\")\\n', 'plot_numeric_cols <- setdiff(numeric_cols, exclude_cols)\\n', '\\n', '# Prepare data for plotting (long format)\\n', 'hist_data_long <- df %>%\\n', '  select(all_of(plot_numeric_cols)) %>%\\n', '  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\")\\n', '\\n', '# Create histograms for each selected numeric variable\\n', 'histogram_plot <- ggplot(hist_data_long, aes(x = value)) +\\n', '  geom_histogram(binwidth = 0.5, fill = \"steelblue\", color = \"black\") +\\n', '  facet_wrap(~ variable, scales = \"free\") +\\n', '  labs(\\n', '    title = \"Histograms of Numerical Variables\",\\n', '    x = \"Value\",\\n', '    y = \"Frequency\"\\n', '  ) +\\n', '  theme_minimal()\\n', '\\n', 'print(histogram_plot)\\n']], ['markdown', ['ðŸ“Š Distribution Insights\\n', '\\n', '- Skewed: air quality, PM2.5, heat wave days, GDP, population\\n', '- Bimodal: mental health, food security\\n', '- Near-normal: temperature, precipitation, respiratory disease\\n', '\\n', '\\n', 'ðŸ§  Modeling Implications\\n', '- Apply log or Box-Cox transformations to skewed variables before regression or tree-based modeling.\\n', '- Use standardization for variables with wide ranges (e.g. GDP, population) to improve model stability.\\n', '- Consider clustering or segmentation for bimodal variables to uncover latent groups.\\n', '- Flag outliers for robust evaluation and scenario testing during deployment.\\n']], ['markdown', ['## Correlation Matrix']], ['code', ['# Select numeric columns\\n', 'selected_cols <- c(\\n', '  \"temperature_celsius\", \"precipitation_mm\", \"heat_wave_days\",\\n', '  \"extreme_weather_events\", \"air_quality_index\",\\n', '  \"vector_disease_risk_score\", \"heat_related_admissions\",\\n', '  \"respiratory_disease_rate\", \"healthcare_access_index\",\\n', '  \"food_security_index\"\\n', ')\\n', '\\n', '# Prepare correlation matrix\\n', 'cor_matrix <- cor(df[, selected_cols], use = \"complete.obs\")\\n', '\\n', '# Convert to long format for ggplot\\n', 'cor_data <- melt(cor_matrix)\\n', '\\n', '# Plot heatmap\\n', 'ggplot(cor_data, aes(x = Var1, y = Var2, fill = value)) +\\n', '  geom_tile() +\\n', '  geom_text(aes(label = sprintf(\"%.2f\", value)), color = \"black\", size = 3) +\\n', '  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\\n', '  theme_minimal() +\\n', '  theme(\\n', '    axis.text.x = element_text(angle = 45, hjust = 1, color = \"black\"),\\n', '    axis.text.y = element_text(color = \"black\")\\n', '  ) +\\n', '  labs(\\n', '    title = \"Correlation Heatmap of Climate Variables\",\\n', '    x = \"\",\\n', '    y = \"\"\\n', '  )']], ['markdown', ['ðŸ“Š Correlation Highlights\\n', '- Temp â†” Vector disease risk (0.65)\\n', '- Temp â†” Heat-related admissions (0.42)\\n', '- Heat wave days â†” Extreme events (0.96) â†’ redundant\\n', '- Healthcare access & food security negatively correlated with climate stressors\\n', '\\n', 'ðŸ§  Modeling & Deployment Tips\\n', '- Prioritize temperature, healthcare access, and vector risk as core features.\\n', '- Use interaction terms (e.g. temperature Ã— access index) to capture compounding effects.\\n', '- Evaluate model fairness across low-access regions.\\n', '- Highlight vulnerable zones in dashboards for policy targeting.\\n']], ['markdown', ['# Regression model\\n']], ['markdown', ['## Install Required Packages for Regression Models\\n', '\\n', 'This section need to run for 20mins+, Require the progress statement to prevent the R runtime disconnection due to false sense of inactivity']], ['code', ['# SECTION: PACKAGE INSTALLATION & ENVIRONMENT SETUP\\n', '\\n', '# 1. Define required packages (Excludes pre-installed dplyr/ggplot2)\\n', 'packages <- c(\"caret\", \"glmnet\", \"ranger\", \"xgboost\", \"recipes\")\\n', '\\n', '# 2. Configure Repositories for High-Speed Linux Binaries\\n', 'options(\\n', '  repos = c(CRAN = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"),\\n', '  Ncpus = parallel::detectCores() # Automatically uses 2 cores (or more if available)\\n', ')\\n', '\\n', '# 3. Identify ONLY what is missing\\n', 'missing_pkgs <- packages[!(packages %in% installed.packages()[, \"Package\"])]\\n', '\\n', '# 4. Install missing items in one optimized batch\\n', 'if (length(missing_pkgs) > 0) {\\n', '  cat(\"Fast-installing missing binaries and dependencies:\", paste(missing_pkgs, collapse = \", \"), \"\\\\n\")\\n', '\\n', '  # dependencies = TRUE is the \"secret sauce\" to get all 27+ sub-packages at once\\n', '  install.packages(missing_pkgs, dependencies = TRUE, quiet = TRUE)\\n', '}\\n', '\\n', '# 5. Load everything efficiently\\n', 'library(dplyr)\\n', 'invisible(lapply(packages, library, character.only = TRUE))\\n', '\\n', 'cat(\"----------------------------------------------------------\\\\n\")\\n', 'cat(\"SUCCESS: Environment Ready (using\", parallel::detectCores(), \"cores)\\\\n\")']], ['markdown', ['## Step 1: Prepare Data for Regression\\n', '\\n', '- Validation & Cleaning: Applied logical range filters to environmental and economic variables (e.g., -50Â°C to 60Â°C for temperature) and discarded 2025 data to ensure a clean, historical dataset.\\n', '\\n', '- Categorical Encoding: Converted region into a factor and income_level into an ordered factor, preserving the natural economic hierarchy (Low to High).\\n', '\\n', '- Feature Engineering: Applied log1p transformations to GDP and Air Quality (AQI) variables to normalize right-skewed data and improve model stability.\\n', '\\n', '- Feature Selection: Curated a final set of 11 predictors (climate, pollution, and socioeconomic metrics) and used na.omit() to ensure a \"complete case\" dataset.\\n', '\\n', '- Stratified Splitting: Partitioned the data into an 80/20 train-test split using stratified sampling to ensure both sets have a similar distribution of heat-related admissions.']], ['code', ['# --- 1. Filter and Clean Data ---\\n', 'df_cleaned_for_regression <- df %>%\\n', '  filter(\\n', '    # Core target validation\\n', '    if(\"heat_related_admissions\" %in% colnames(.)) heat_related_admissions >= 0 | is.na(heat_related_admissions) else TRUE,\\n', '\\n', '    # Predictor validations\\n', '    if(\"temperature_celsius\" %in% colnames(.)) temperature_celsius >= -50 & temperature_celsius <= 60 else TRUE,\\n', '    if(\"temp_anomaly_celsius\" %in% colnames(.)) temp_anomaly_celsius >= -20 & temp_anomaly_celsius <= 20 else TRUE,\\n', '    if(\"heat_wave_days\" %in% colnames(.)) heat_wave_days >= 0 & heat_wave_days <= 366 else TRUE,\\n', '    if(\"air_quality_index\" %in% colnames(.)) air_quality_index >= 0 & air_quality_index <= 500 else TRUE,\\n', '    if(\"healthcare_access_index\" %in% colnames(.)) healthcare_access_index >= 0 & healthcare_access_index <= 100 else TRUE,\\n', '    if(\"gdp_per_capita_usd\" %in% colnames(.)) gdp_per_capita_usd >= 0 else TRUE\\n', '  )\\n', '\\n', '# --- 1.1 Discard 2025 data ---\\n', 'df_clean <- df_cleaned_for_regression %>% filter(year < 2025)\\n', '\\n', '# --- 1.2 Convert categorical variables to factors ---\\n', 'df_clean$region <- as.factor(df_clean$region)\\n', 'df_clean$income_level <- factor(df_clean$income_level,\\n', '                                levels = c(\"Low\", \"Lower-Middle\", \"Upper-Middle\", \"High\"), # Added \\'Low\\' just in case\\n', '                                ordered = TRUE)\\n', '\\n', '# --- 1.3 Feature Engineering & Transformations ---\\n', '# Using log1p (log(1+x)) to handle skewed environmental and economic data\\n', 'df_clean$log_gdp <- log1p(df_clean$gdp_per_capita_usd)\\n', 'df_clean$log_pm25 <- log1p(df_clean$pm25_ugm3)\\n', 'df_clean$log_aqi <- log1p(df_clean$air_quality_index)\\n', '\\n', '# --- 1.4 Select Features for Regression ---\\n', '# UPDATED: Included heat_related_admissions and removed respiratory_disease_rate\\n', 'features <- c(\"temperature_celsius\", \"temp_anomaly_celsius\", \"precipitation_mm\",\\n', '              \"heat_wave_days\", \"log_pm25\", \"log_aqi\", \"healthcare_access_index\",\\n', '              \"log_gdp\", \"region\", \"income_level\")\\n', '\\n', '# We must select the target variable explicitly to use it in df_model\\n', 'df_model <- df_clean %>%\\n', '  select(all_of(features), heat_related_admissions) %>%\\n', '  na.omit()\\n', '\\n', '# --- 1.5 Split Data (80/20) ---\\n', 'set.seed(123)\\n', '# Splitting based on the heat_related_admissions distribution\\n', 'trainIndex_reg <- createDataPartition(df_model$heat_related_admissions, p = .8, list = FALSE)\\n', 'train_data_reg <- df_model[trainIndex_reg, ]\\n', 'test_data_reg  <- df_model[-trainIndex_reg, ]\\n', '\\n', 'cat(\"Environment setup for Heat Related Admissions complete! âœ“\\\\n\")\\n', 'cat(\"Training set size:\", nrow(train_data_reg), \"rows\\\\n\")']], ['markdown', ['## Step 2: Configure Cross-Validation\\n', '\\n', 'Set up 5-fold cross-validation for model training to ensure the model generalizes well and to prevent overfitting.']], ['code', ['# Set up 5-fold cross-validation\\n', 'train_control <- trainControl(method = \"cv\", number = 5, verboseIter = TRUE)']], ['markdown', ['## Step 3: Train Regression Models\\n', '\\n', 'Hardware Optimization: Automatically detected CPU capacity and allocated parallel threads (n_cores - 1) to maximize processing speed for ensemble models.\\n', '\\n', 'Data Transformation: Implemented One-Hot Encoding via dummyVars to convert categorical regions and income levels into numeric matrices, ensuring compatibility across all model types.\\n', '\\n', 'Based on research objectives, we will train three distinct models:\\n', '\\n', '- Model 1 (Elastic Net): Established a robust linear baseline using glmnet to manage multicollinearity between correlated climate variables (e.g., Temperature vs. Heat Wave Days).\\n', '\\n', '- Model 2 (Random Forest): Utilized the ranger implementation to capture complex, non-linear interactions between socioeconomic factors and environmental stressors.\\n', '\\n', '- Model 3 (XGBoost): Implemented a high-performance Gradient Boosting approach using a native DMatrix structure for memory efficiency and maximum predictive accuracy.\\n', '\\n', '\\n', '\\n', '\\n', 'Training Safeguards: Integrated Early Stopping (10 rounds) in XGBoost and 5-fold Cross-Validation in caret to prevent overfitting and ensure the models generalize well to new heat admission data.']], ['code', ['# Re-detect hardware capacity\\n', 'library(parallel)\\n', 'n_cores <- detectCores()\\n', '# Define the missing variable\\n', 'suggested_threads <- max(1, n_cores - 1)\\n', '\\n', 'cat(\"Hardware Check: Using\", suggested_threads, \"threads for training.\\\\n\")\\n', '\\n', '\\n', '# --- One-Hot Encoding ---\\n', 'dummy_transformer <- dummyVars(heat_related_admissions ~ ., data = df_model)\\n', '\\n', '# Standardize as matrices immediately to avoid repeating the conversion inside models\\n', 'to_plain_matrix <- function(transformer, data) {\\n', '  tmp <- predict(transformer, newdata = data)\\n', '  mat <- matrix(as.numeric(tmp), nrow = nrow(tmp), ncol = ncol(tmp))\\n', '  colnames(mat) <- colnames(tmp)\\n', '  return(mat)\\n', '}\\n', '\\n', 'train_x <- to_plain_matrix(dummy_transformer, train_data_reg)\\n', 'test_x  <- to_plain_matrix(dummy_transformer, test_data_reg)\\n', '\\n', 'train_y <- train_data_reg$heat_related_admissions\\n', 'test_y  <- test_data_reg$heat_related_admissions\\n', '\\n', 'dtrain <- xgb.DMatrix(data = train_x, label = train_y)\\n', 'dtest  <- xgb.DMatrix(data = test_x, label = test_y)\\n', '\\n', '# --- Step 3: Train Regression Models ---\\n', '\\n', '# Model 1: Elastic Net\\n', 'cat(\"Training Model 1: Elastic Net...\\\\n\")\\n', 'time_enet <- system.time({\\n', '  reg_model_enet <- train(\\n', '    x = train_x, y = train_y,\\n', '    method = \"glmnet\",\\n', '    trControl = train_control\\n', '  )\\n', '})\\n', 'cat(\"Elapsed Time:\", time_enet[\"elapsed\"], \"seconds\\\\nModel 1 Complete! âœ“\\\\n\\\\n\")\\n', '\\n', '# Model 2: Ranger (Random Forest)\\n', 'cat(\"Training Model 2: Ranger Forest...\\\\n\")\\n', 'time_rf <- system.time({\\n', '  reg_model_rf <- train(\\n', '    x = train_x, y = train_y,\\n', '    method = \"ranger\",\\n', '    trControl = train_control,\\n', '    num.trees = 100,\\n', '    num.threads = suggested_threads,\\n', '    importance = \"impurity\"\\n', '  )\\n', '})\\n', 'cat(\"Elapsed Time:\", time_rf[\"elapsed\"], \"seconds\\\\nModel 2 Complete! âœ“\\\\n\\\\n\")\\n', '\\n', '# Model 3: XGBoost\\n', '\\n', 'params <- list(\\n', '  booster = \"gbtree\",\\n', '  objective = \"reg:squarederror\",\\n', '  eta = 0.3,\\n', '  max_depth = 6,\\n', '  nthread = suggested_threads\\n', ')\\n', '\\n', '\\n', 'cat(\"Training Model 3: XGBoost...\\\\n\")\\n', 'time_xgb <- system.time({\\n', '  reg_model_xgb <- xgb.train(\\n', '    params = params,\\n', '    data = dtrain,\\n', '    nrounds = 100,\\n', '    watchlist = list(val=dtest, train=dtrain),\\n', '    print_every_n = 10,\\n', '    early_stopping_rounds = 10,\\n', '    maximize = FALSE\\n', ')\\n', '})\\n', 'cat(\"Elapsed Time:\", time_xgb[\"elapsed\"], \"seconds\\\\nModel 3 Complete! âœ“\\\\n\\\\n\")\\n', '\\n', 'cat(\"\\\\nAll models trained successfully! âœ“\\\\n\")']], ['code', ['# Create a timing summary\\n', 'timing_summary <- data.frame(\\n', '  Model = c(\"Elastic Net\", \"Ranger\", \"XGBoost\"),\\n', '  Time_Seconds = c(time_enet[\"elapsed\"], time_rf[\"elapsed\"], time_xgb[\"elapsed\"])\\n', ')\\n', '\\n', 'print(timing_summary)']], ['markdown', ['## Step 4: Compare Model Performance (Cross-Validation)\\n', '\\n', 'Compare accuracy across models using cross-validation results\\n', '\\n', '1. Metric Aggregation: Utilized the resamples() function to collect and align performance distributions (RMSE, $R^2$, MAE) from the cross-validation folds.\\n', '\\n', '2. Performance Benchmarking: Generated a statistical summary to compare the average error and stability of the Elastic Net and Random Forest models.\\n', '\\n', '3. Visual Comparative Analysis:\\n', \"    - dotplot: Used to visualize the confidence intervals of each model's accuracy, identifying which model is more consistent\\n\", '    - .bwplot (Boxplot): Employed to analyze the variance of the RMSE,highlighting the risk of error fluctuations across different data folds.\\n', '\\n', '4. Relative Evaluation: While XGBoost is evaluated separately on the test set, these CV metrics provide the primary evidence for the Random Forest and Elastic Net reliability before final testing.']], ['code', ['cat(\"\\\\n Comparing Model Performance (CV Results)...\\\\n\")\\n', '\\n', '# 1. Collect results ONLY from caret models\\n', '# XGBoost is native now, so it cannot go into resamples()\\n', 'results <- resamples(list(\\n', '  ElasticNet = reg_model_enet,\\n', '  RandomForest = reg_model_rf\\n', '))\\n', '\\n', '# 2. Print summary and plots for the caret models\\n', 'summary_results <- summary(results)\\n', 'print(summary_results)\\n', '\\n', 'dotplot(results, main = \"Caret Models: Cross-Validation Metrics\")\\n', 'bwplot(results, main = \"Caret Models: RMSE Distribution\")']], ['markdown', ['## Step 5: Make prediction & Evaluate Performance of the test data\\n', '\\n', '- Custom Metric Standardization: Implemented a dedicated eval_metrics function to compute a multi-dimensional performance profile, including RMSE for error magnitude, MAE for average prediction deviation, and $R^2$ for the proportion of variance explained.\\n', '\\n', '- Final Predictive Validation: Executed model predictions on a \"hold-out\" test set (20% of the data) that was never seen during training, providing an unbiased assessment of how the models will perform on real-world future data.\\n', '\\n', '- Ensemble \"Horse Race\": Simultaneously evaluated the Elastic Net, Random Forest, and XGBoost models to statistically identify the most reliable architecture for heat-related admission forecasting.\\n', '\\n', '- Visual Error Analysis: Utilized a Predicted vs. Actual scatter plot to diagnose model behavior.\\n', '\\n', '  - Identity Mapping: Added a red dashed $1:1$ line to represent the \"perfect model\" baseline.\\n', '\\n', '  - Bias Detection: Used the distribution of points around the line to confirm that the model remains accurate across both low and high admission counts.']], ['code', ['cat(\"\\\\n Making Predictions and Evaluating on Test Set...\\\\n\")\\n', '\\n', '# Function to calculate regression metrics\\n', 'eval_metrics <- function(actual, predicted) {\\n', '  rmse <- sqrt(mean((actual - predicted)^2))\\n', '  mae  <- mean(abs(actual - predicted))\\n', '  rss  <- sum((actual - predicted)^2)\\n', '  tss  <- sum((actual - mean(actual))^2)\\n', '  rsq  <- 1 - (rss/tss)\\n', '  return(c(RMSE = rmse, R2 = rsq, MAE = mae))\\n', '}\\n', '\\n', \"# Ensure test_x is a matrix if it isn't already\\n\", 'test_x_matrix <- as.matrix(test_x)\\n', '\\n', '# Predict using all three models\\n', 'pred_enet <- predict(reg_model_enet, test_x_matrix)\\n', 'pred_rf   <- predict(reg_model_rf, test_x_matrix)\\n', 'pred_xgb  <- predict(reg_model_xgb, dtest)\\n', '\\n', '# Create a comparison table for Test Set\\n', 'test_eval <- data.frame(\\n', '  Model = c(\"Elastic Net\", \"Random Forest\", \"XGBoost\"),\\n', '  rbind(\\n', '    eval_metrics(test_y, pred_enet),\\n', '    eval_metrics(test_y, pred_rf),\\n', '    eval_metrics(test_y, pred_xgb)\\n', '  )\\n', ')\\n', '\\n', 'print(test_eval)\\n', '\\n', 'library(ggplot2)\\n', '\\n', '# Combine results for plotting\\n', 'plot_data <- data.frame(\\n', '  Actual = test_y,\\n', '  XGBoost = pred_xgb,\\n', '  RandomForest = pred_rf\\n', ')\\n', '\\n', '# Plot for the XGBoost Model\\n', 'ggplot(plot_data, aes(x = Actual, y = XGBoost)) +\\n', '  geom_point(alpha = 0.4, color = \"darkblue\") +\\n', '  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", size = 1) +\\n', '  labs(title = \"XGBoost: Predicted vs. Actual Heat Admissions\",\\n', '       subtitle = paste(\"R-Squared:\", round(test_eval[test_eval$Model==\"XGBoost\", \"R2\"], 3)),\\n', '       x = \"Actual Admissions\",\\n', '       y = \"Predicted Admissions\") +\\n', '  theme_minimal()']], ['markdown', ['## Step 6: Identify Best Model and Visualize Results\\n', '\\n', '- Automated Selection Logic: Implemented an objective selection process that programmatically identifies the winner by finding the minimum Root Mean Square Error (RMSE) among all candidates.\\n', '\\n', '- Dynamic Prediction Mapping: The script automatically maps the correct model object and its corresponding test-set predictions for downstream analysis, ensuring the visualizations always reflect the highest-performing architecture.\\n', '\\n', '- Regression Accuracy Visualization: Produced a \"Predicted vs. Actual\" scatter plot to assess model bias.\\n', '\\n', '  - Steelblue Points: Represent individual admission records.\\n', '\\n', '  - Red Dashed Line: Acts as the 1:1 \"perfect prediction\" reference.\\n', '\\n', '- Factor Influence Analysis (Variable Importance): Deployed a conditional importance logic tailored to the model type (native xgb.importance for XGBoost or varImp for Caret models).\\n', '\\n', \"  - Insight: This identifies which environmental or socioeconomic driversâ€”such as Heat Wave Days or Temperatureâ€”had the most weight in the model's decision-making process.\\n\", '\\n', '- Interpretability: By ranking the top 10 influencing factors, the approach transitions from \"black-box\" prediction to actionable scientific insight, specifically highlighting how climate stressors impact hospital workload.']], ['code', ['cat(\"\\\\n Visualizing Best Model Results...\\\\n\")\\n', '\\n', '# 1. Identify the best model automatically\\n', 'best_model_idx <- which.min(test_eval$RMSE)\\n', 'best_model_name <- test_eval$Model[best_model_idx]\\n', 'cat(paste0(\"The best performing model is: \", best_model_name, \"\\\\n\"))\\n', '\\n', '# 2. Select the correct model object and predictions\\n', 'if(best_model_name == \"XGBoost\") {\\n', '  best_model_obj <- reg_model_xgb\\n', '  best_preds     <- pred_xgb\\n', '} else if(best_model_name == \"Random Forest\") {\\n', '  best_model_obj <- reg_model_rf\\n', '  best_preds     <- pred_rf\\n', '} else {\\n', '  best_model_obj <- reg_model_enet\\n', '  best_preds     <- pred_enet\\n', '}\\n', '\\n', '# 3. Regression Plot: Predicted vs Actual\\n', 'plot_data <- data.frame(Actual = test_y, Predicted = best_preds)\\n', '\\n', 'library(ggplot2)\\n', 'p1 <- ggplot(plot_data, aes(x = Actual, y = Predicted)) +\\n', '  geom_point(alpha = 0.5, color = \"steelblue\") +\\n', '  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\\n', '  labs(title = paste(\"Best Model Performance:\", best_model_name),\\n', '       subtitle = \"Actual vs Predicted Heat Related Admissions\",\\n', '       x = \"Actual Admissions\",\\n', '       y = \"Predicted Admissions\") +\\n', '  theme_minimal()\\n', '\\n', 'print(p1)\\n', '\\n', '# 4. Corrected Variable Importance Plot\\n', 'cat(\"\\\\n Generating Variable Importance for:\", best_model_name, \"...\\\\n\")\\n', '\\n', 'if(best_model_name == \"XGBoost\") {\\n', '  # Native XGBoost importance logic\\n', '  imp_mat <- xgb.importance(model = reg_model_xgb)\\n', '  xgb.plot.importance(imp_mat, top_n = 10, main = \"XGBoost Variable Importance\")\\n', '} else {\\n', '  # Caret importance logic for RF and E-Net\\n', '  importance <- varImp(best_model_obj)\\n', '  plot(importance, main = paste(\"Top Factors Influencing Admissions (\", best_model_name, \")\", sep=\"\"))\\n', '}']], ['markdown', ['## Regression Results Summary\\n', '\\n', '1. **Training Environment and Data**\\n', '    - Training Scale: The models were trained on a final dataset of 10,162 rows.\\n', '\\n', '    - Methodology: Used 5-fold cross-validation (CV) to ensure stability across different data subsets.\\n', '\\n', '    - Computing: Optimized for a single-thread environment during training.\\n', '\\n', '\\n', '2. **Performance Comparison**\\n', '    - The Random Forest (Ranger) model was identified as the best-performing model, outperforming both the linear baseline (Elastic Net) and the high-speed gradient booster (XGBoost).\\n', '    - Accuracy: The Random Forest model explains approximately 82% of the variance in heat-related admissions on the test set.\\n', \"    - Error Rate: On average, the Random Forest model's predictions were off by only 2.55 admissions (MAE), compared to 4.04 for the Elastic Net.\\n\", '    - Stability: Cross-validation plots confirm that Random Forest maintained a significantly lower and tighter error distribution (RMSE) than the Elastic Net.\\n', '\\n', '3. **Predictive Insights (XGBoost Analysis)**\\n', '\\n', \"    While Random Forest won on accuracy, the XGBoost visualization provides clear evidence of the model's reliability:\\n\", '\\n', '    -  Correlation: The \"Predicted vs. Actual\" scatter plot shows a strong alignment with the red identity line, indicating that predictions closely follow actual trends.\\n', '    - Performance: Achieved an $R^2$ of 0.801 on the test set, demonstrating that even the second-best model remains highly competitive.\\n', '\\n', '4. **Key Drivers of Heat Admissions**\\n', '\\n', '    The Variable Importance analysis for the winning Random Forest model highlights the critical factors influencing hospital admissions:\\n', '    \\n', \"    - Heat Wave Days: By far the most significant predictor, dominating the model's decision-making.\\n\", '    - Temperature (Celsius): The second strongest environmental driver.\\n', '    - Precipitation (mm): Shows a notable influence, potentially interacting with humidity.\\n', '    - Log GDP & Healthcare Access: Socioeconomic factors serve as secondary but important predictors of admissions.Regional/Income Factors: Geographic and economic categories (e.g., region.Europe) showed the lowest relative influence compared to direct environmental stressors.\\n', '\\n', '5. **Efficiency Summary**\\n', '    - **XGBoost** was the fastest to train (0.44 seconds), offering a near-perfect balance between speed and high accuracy (R^2 = 0.80).\\n', '    - **Random Forest** required the most time (119.7 seconds) but delivered the most precise predictions for this specific heat-health dataset.']], ['markdown', ['# Classification model\\n', '\\n', '**Goal:**\\n', '\\n', 'Predict population health risk levels using climate, air quality, and socioeconomic indicators, focusing on identifying populations at higher risk from climate-sensitive health outcomes. For modeling purposes, health risk was binarized into High vs Not_High to simplify prediction, ensure stable model training, and emphasize populations in the top one-third of risk scores.\\n', '\\n', '**Approach:**\\n', '- 3 classification models (Logistic Regression, Random Forest, and k-Nearest Neighbors (kNN)) were trained to classify health risk.\\n', '- The dataset was split into 80% training and 20% testing sets.\\n', '- 5-fold cross-validation was applied on the training data, with ROC as the primary metric to handle potential class imbalance and select the best model.\\n', '- Final model performance was evaluated on the unseen test set using accuracy, sensitivity, specificity, balanced accuracy, and confusion matrices.\\n', '- Binarizing the target allows the models to focus on distinguishing high-risk populations from the rest, which is most relevant for policy and health intervention decisions.']], ['markdown', ['## Install Required Packages for Classification Models']], ['code', ['# Install required packages for classification models\\n', 'if (!require(nnet)) install.packages(\"nnet\", repos = \"https://cran.rstudio.com/\")\\n', 'if (!require(randomForest)) install.packages(\"randomForest\", repos = \"https://cran.rstudio.com/\")\\n', 'if (!require(class)) install.packages(\"class\", repos = \"https://cran.rstudio.com/\")\\n', '\\n', '# Load the packages\\n', 'library(nnet)         # For multinomial logistic regression\\n', 'library(randomForest) # For random forest\\n', 'library(class)        # For k-NN\\n', '\\n', 'cat(\"All classification packages loaded successfully! âœ“\\\\n\")']], ['markdown', ['## Step 1: Data Preparation for Classification\\n', '\\n', '- A binary target variable (health_risk_binary) was created based on a composite health risk score from standardized respiratory disease rate, heat-related admissions, and cardiovascular mortality.\\n', '- Observations above the 66th percentile were labeled as **High**, others as **Not_High**.\\n', '  - Reason: The 66th percentile ensures that roughly the top one-third of health risk scores are classified as High, highlighting populations at higher risk while keeping enough cases in each class for stable model training.\\n', '- Predictor features included climate (temperature, precipitation, heat waves, extreme weather), air quality (AQI, PM2.5), and socioeconomic indicators (GDP per capita, income level, healthcare access, food security).\\n', '- Categorical variables were converted to factors.\\n', '- The dataset was split into 80% training and 20% testing for unbiased evaluation.']], ['code', ['# 1. Create Binary Target Variable\\n', '# Convert health outcomes into a binary classification problem to ensure model stability in Google Colab\\n', '# Classify whether a country-week observation represents High Health Risk or Not High Risk\\n', '\\n', '# Create composite health risk score\\n', 'df <- df %>%\\n', '  mutate(\\n', '    health_risk_score =\\n', '      scale(respiratory_disease_rate) +\\n', '      scale(heat_related_admissions) +\\n', '      scale(cardio_mortality_rate)\\n', '  )\\n', '\\n', '# Convert into binary classification\\n', 'df <- df %>%\\n', '  mutate(\\n', '    health_risk_binary = ifelse(\\n', '      health_risk_score > quantile(health_risk_score, 0.66),\\n', '      \"High\",\\n', '      \"Not_High\"\\n', '    )\\n', '  )\\n', '\\n', '# Convert to factor\\n', 'df$health_risk_binary <- factor(df$health_risk_binary)']], ['code', ['# 2. Select features\\n', '# Use climate, air quality, and socioeconomic indicators as predictors\\n', '\\n', 'classification_data <- df %>%\\n', '  select(\\n', '    country_name,\\n', '    year,\\n', '    temperature_celsius,\\n', '    precipitation_mm,\\n', '    heat_wave_days,\\n', '    extreme_weather_events,\\n', '    air_quality_index,\\n', '    pm25_ugm3,\\n', '    healthcare_access_index,\\n', '    gdp_per_capita_usd,\\n', '    food_security_index,\\n', '    income_level,\\n', '    health_risk_binary\\n', '  )\\n', '\\n', '\\n', '# Convert categorical variable\\n', 'classification_data$income_level <- factor(classification_data$income_level)']], ['code', ['# 3. Split Data into Training and Test Sets (80/20)\\n', '# Evaluate model performance on unseen data\\n', '\\n', 'set.seed(123)\\n', '\\n', 'train_index <- createDataPartition(\\n', '  classification_data$health_risk_binary,\\n', '  p = 0.8,\\n', '  list = FALSE\\n', ')\\n', '\\n', 'train_data <- classification_data[train_index, ]\\n', 'test_data  <- classification_data[-train_index, ]']], ['markdown', ['## Step 2: Cross-Validation Configuration\\n', '\\n', 'To ensure robust and reliable model performance estimation\\n', '\\n', '- 5-fold cross-validation was applied during model training. The training data was divided into 5 subsets, with 4 folds used for training and 1 fold for validation in each iteration.\\n', '- This approach reduces overfitting and provides a more stable estimate of model performance compared to a single train-test split.\\n', '- ROC (Receiver Operating Characteristic) was used to select the best model, as it effectively measures the ability to distinguish High-risk from Not_High-risk observations in potentially imbalanced data.']], ['code', ['ctrl <- trainControl(\\n', '  method = \"cv\",\\n', '  number = 5,\\n', '  classProbs = TRUE,\\n', '  summaryFunction = twoClassSummary\\n', ')']], ['markdown', ['## Step 3: Model Training\\n', '\\n', '3 models were trained on the training set:\\n', '- **Logistic Regression**: interpretable baseline model assuming linear relationships\\n', '- **Random Forest**: ensemble model capable of capturing non-linear interactions and variable importance\\n', '- **k-Nearest Neighbors (kNN)**: distance-based model requiring feature scaling\\n', '\\n', 'Identifier variables such as country name and year were excluded to avoid data leakage']], ['code', ['# Logistic Regression\\n', 'model_logit <- train(\\n', '  health_risk_binary ~ . -country_name -year,  # Exclude identifiers\\n', '  data = train_data,\\n', '  method = \"glm\",\\n', '  family = \"binomial\",\\n', '  trControl = ctrl,\\n', '  metric = \"ROC\"\\n', ')\\n', '\\n', '# Random Forest\\n', 'model_rf <- train(\\n', '  health_risk_binary ~ . -country_name -year,\\n', '  data = train_data,\\n', '  method = \"rf\",\\n', '  trControl = ctrl,\\n', '  metric = \"ROC\",\\n', '  importance = TRUE\\n', ')\\n', '\\n', '# kNN\\n', 'model_knn <- train(\\n', '  health_risk_binary ~ . -country_name -year,\\n', '  data = train_data,\\n', '  method = \"knn\",\\n', '  trControl = ctrl,\\n', '  preProcess = c(\"center\", \"scale\"),\\n', '  tuneLength = 10,\\n', '  metric = \"ROC\"\\n', ')']], ['markdown', ['## Step 4: Cross-Validation Performance Comparison\\n', 'Cross-validated ROC scores were compared to assess model performance and stability across folds. Logistic Regression achieved the highest mean ROC, followed closely by Random Forest, while kNN showed slightly lower performance.\\n', '\\n', 'Considering ROC stability and the ability to model complex relationships, Random Forest was selected as the final model.']], ['code', ['resamples_results <- resamples(\\n', '  list(\\n', '    Logistic = model_logit,\\n', '    RandomForest = model_rf,\\n', '    kNN = model_knn\\n', '  )\\n', ')\\n', '\\n', 'summary(resamples_results)\\n', 'bwplot(resamples_results, metric = \"ROC\")']], ['markdown', ['Logistic Regression achieved the highest mean ROC (~ 0.81), followed closely by Random Forest (~ 0.80) and kNN (~ 0.79). Sensitivity is modest (~ 0.45â€“0.51), indicating some high-risk cases are missed, but specificity is high (>0.90), meaning low-risk populations are accurately identified. Random Forest balances ROC stability and ability to handle non-linear relationships, making it the best choice for deployment.â€']], ['markdown', ['## Step 5: Test Data Predictions & Confusion Matrix\\n', 'The selected models were evaluated on the unseen test dataset using accuracy, sensitivity, specificity, and balanced accuracy. These metrics provide insight into the modelâ€™s ability to correctly identify both high-risk and low-risk populations.']], ['code', ['pred_logit <- predict(model_logit, test_data)\\n', 'pred_rf    <- predict(model_rf, test_data)\\n', 'pred_knn   <- predict(model_knn, test_data)']], ['markdown', ['- Confusion matrices were generated to calculate overall accuracy and class-level performance.\\n', '- Key metrics include sensitivity (correctly identifying High-risk cases), specificity (correctly identifying Not_High-risk), and balanced accuracy.']], ['code', ['confusionMatrix(pred_logit, test_data$health_risk_binary)\\n', 'confusionMatrix(pred_rf, test_data$health_risk_binary)\\n', 'confusionMatrix(pred_knn, test_data$health_risk_binary)']], ['markdown', ['Although Logistic Regression achieved slightly higher accuracy (76.8%) and balanced accuracy (70%) compared to Random Forest, the differences in performance were marginal. Random Forest achieved an accuracy of approximately 76.6%, with high specificity (>92%) and moderate sensitivity (~45%). Random Forest was selected as the final model because it is better suited to capturing non-linear relationships and interactions among climate, air quality, and socioeconomic variables. Climateâ€“health relationships are inherently complex and non-linear, and Random Forest provides greater modeling flexibility while maintaining comparable predictive performance. Additionally, Random Forest enables feature importance analysis, which supports interpretability and policy-relevant insights.']], ['code', ['accuracy_results <- data.frame(\\n', '  Model = c(\"Logistic Regression\", \"Random Forest\", \"kNN\"),\\n', '  Accuracy = c(\\n', '    confusionMatrix(pred_logit, test_data$health_risk_binary)$overall[\"Accuracy\"],\\n', '    confusionMatrix(pred_rf, test_data$health_risk_binary)$overall[\"Accuracy\"],\\n', '    confusionMatrix(pred_knn, test_data$health_risk_binary)$overall[\"Accuracy\"]\\n', '  )\\n', ')\\n', '\\n', 'ggplot(accuracy_results, aes(x = Model, y = Accuracy, fill = Model)) +\\n', '  geom_bar(stat = \"identity\") +\\n', '  geom_text(aes(label = sprintf(\"%.2f\", Accuracy)), vjust = -0.5) +  # show accuracy above bars\\n', '  ylim(0, 1) +  # fix y-axis from 0 to 1 for clarity\\n', '  theme_minimal() +\\n', '  labs(title = \"Model Accuracy Comparison\", y = \"Accuracy\", x = \"\")']], ['markdown', ['The bar chart confirms that all models perform similarly, with small differences in accuracy. Confusion matrices indicate trade-offs between sensitivity and specificity that should guide model selection depending on whether identifying High-risk populations is more critical than avoiding false positives.']], ['code', ['cm <- confusionMatrix(pred_rf, test_data$health_risk_binary)$table\\n', 'cm_df <- as.data.frame(cm)\\n', 'colnames(cm_df) <- c(\"Predicted\", \"Actual\", \"Count\")\\n', '\\n', 'ggplot(cm_df, aes(x = Actual, y = Predicted, fill = Count)) +\\n', '  geom_tile() +\\n', '  geom_text(aes(label = Count), color = \"white\", size = 5) +\\n', '  scale_fill_gradient(low = \"steelblue\", high = \"firebrick\") +\\n', '  labs(\\n', '    title = \"Confusion Matrix â€“ Random Forest\",\\n', '    x = \"Actual Class\",\\n', '    y = \"Predicted Class\"\\n', '  ) +\\n', '  theme_minimal()']], ['code', ['library(pROC)\\n', '\\n', '# ROC curve for Random Forest\\n', 'rf_probs <- predict(model_rf, test_data, type = \"prob\")\\n', '\\n', 'roc_rf <- roc(\\n', '  response = test_data$health_risk_binary,\\n', '  predictor = rf_probs$High,\\n', '  levels = rev(levels(test_data$health_risk_binary))\\n', ')\\n', '\\n', 'plot(\\n', '  roc_rf,\\n', '  main = \"ROC Curve â€“ Random Forest Classifier\",\\n', '  col = \"darkblue\",\\n', '  lwd = 2\\n', ')\\n', '\\n', 'auc(roc_rf)']], ['markdown', ['The ROC curve demonstrates strong discrimination between High-risk and Not High-risk populations, with an AUC of approximately 0.80. This indicates good overall classification performance beyond random chance.']], ['markdown', ['## Step 6: Feature Importance (Random Forest)\\n', 'Feature importance from the Random Forest model was examined to identify the most influential predictors of high health risk.']], ['code', ['varImp(model_rf)']], ['code', ['# Extract feature importance\\n', 'importance_rf <- varImp(model_rf, scale = TRUE)\\n', '\\n', '# Convert importance to data frame safely\\n', 'importance_df <- data.frame(\\n', '  Feature = rownames(importance_rf$importance),\\n', '  Importance = importance_rf$importance[, 1]\\n', ')\\n', '\\n', '# Sort by importance\\n', 'importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]\\n', '\\n', '# Plot\\n', 'library(ggplot2)\\n', '\\n', 'ggplot(importance_df,\\n', '       aes(x = reorder(Feature, Importance), y = Importance, fill = Importance)) +\\n', '  geom_bar(stat = \"identity\") +\\n', '  coord_flip() +\\n', '  labs(\\n', '    title = \"Random Forest Feature Importance\",\\n', '    x = \"Feature\",\\n', '    y = \"Relative Importance\"\\n', '  ) +\\n', '  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\\n', '  theme_minimal()']], ['markdown', ['The most important predictors of high health risk were heat wave days, average temperature, and PM2.5 concentration, followed by extreme weather events and air quality index. Socioeconomic variables such as food security, GDP per capita, and healthcare access showed smaller but meaningful contributions.\\n', '\\n', 'These results highlight that climate extremes and air pollution are the dominant drivers of population health risk, while socioeconomic conditions influence resilience and vulnerability.']], ['markdown', ['## Step 8: Identification of High-Risk Populations\\n', '\\n', 'The Random Forest model was used to estimate the probability of each observation belonging to the High-risk class. The top 10 observations with the highest predicted probabilities were identified to highlight populations most vulnerable to climate-sensitive health outcomes.']], ['code', ['# Predicted class labels\\n', 'test_data$predicted_class <- predict(model_rf, test_data)\\n', '\\n', '# Predicted probability of High-risk\\n', 'probs <- predict(model_rf, test_data, type = \"prob\")\\n', 'test_data$predicted_prob_high <- probs$High']], ['code', ['top_high_risk <- test_data %>%\\n', '  arrange(desc(predicted_prob_high)) %>%\\n', '  head(10) %>%\\n', '  select(country_name, year, health_risk_binary, predicted_class, predicted_prob_high)\\n', '\\n', 'top_high_risk']], ['markdown', ['The highest-risk populations were concentrated in the Philippines, Italy, Vietnam, France, and China across multiple years. Repeated high-risk predictions for the Philippines and Italy suggest persistent vulnerability driven by climate extremes and air quality conditions. These findings can help policymakers prioritize targeted health interventions and climate adaptation strategies.']], ['code', ['# Aggregate predicted classes by year\\n', 'risk_trend <- test_data %>%\\n', '  group_by(year, predicted_class) %>%\\n', '  summarise(count = n(), .groups = \"drop\") %>%\\n', '  group_by(year) %>%\\n', '  mutate(percentage = count / sum(count) * 100)\\n', '\\n', '# Plot\\n', 'ggplot(risk_trend, aes(x = year, y = percentage, fill = predicted_class)) +\\n', '  geom_area(alpha = 0.8) +\\n', '  labs(\\n', '    title = \"Predicted Population Health Risk Over Time\",\\n', '    x = \"Year\",\\n', '    y = \"Percentage of Observations\",\\n', '    fill = \"Health Risk\"\\n', '  ) +\\n', '  theme_minimal()']], ['markdown', ['This visualization shows the changing proportion of populations classified as High risk over time. An increasing share of High-risk observations in recent years suggests growing climate-related health vulnerability, highlighting the need for stronger adaptation and healthcare preparedness.']], ['code', ['# Count High-risk predictions by country\\n', 'country_risk <- test_data %>%\\n', '  filter(predicted_class == \"High\") %>%\\n', '  count(country_name, sort = TRUE) %>%\\n', '  top_n(10, n)\\n', '\\n', '# Plot\\n', 'ggplot(country_risk, aes(x = reorder(country_name, n), y = n)) +\\n', '  geom_bar(stat = \"identity\", fill = \"firebrick\") +\\n', '  coord_flip() +\\n', '  labs(\\n', '    title = \"Countries with Most High-Risk Predictions\",\\n', '    x = \"Country\",\\n', '    y = \"Number of High-Risk Observations\"\\n', '  ) +\\n', '  theme_minimal()']], ['markdown', ['While the highest-risk individual observations were concentrated in countries such as the Philippines and Italy, the frequency-based analysis reveals that countries like Indonesia, Kenya, and Bangladesh experience persistent high-risk conditions across multiple weeks. This distinction highlights the difference between extreme health risk events and chronic climate-health vulnerability, both of which are important for public health planning.']], ['code', ['ggplot(test_data, aes(x = predicted_prob_high)) +\\n', '  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.8) +\\n', '  labs(\\n', '    title = \"Distribution of Predicted High-Risk Probabilities\",\\n', '    x = \"Predicted Probability of High Risk\",\\n', '    y = \"Number of Observations\"\\n', '  ) +\\n', '  theme_minimal()']], ['markdown', ['The distribution of predicted probabilities shows that many populations fall within a moderate-risk range, while a smaller group exhibits extremely high risk. This probability-based view allows policymakers to prioritize interventions not only for confirmed high-risk populations but also for those approaching critical risk thresholds.']], ['code', ['ggplot(test_data, aes(\\n', '  x = temperature_celsius,\\n', '  y = pm25_ugm3,\\n', '  color = predicted_class\\n', ')) +\\n', '  geom_point(alpha = 0.6) +\\n', '  labs(\\n', '    title = \"Climate and Air Quality Conditions by Predicted Health Risk\",\\n', '    x = \"Average Temperature (Â°C)\",\\n', '    y = \"PM2.5 Concentration (Âµg/mÂ³)\",\\n', '    color = \"Predicted Risk\"\\n', '  ) +\\n', '  theme_minimal()']], ['markdown', ['High-risk populations tend to cluster in regions experiencing higher temperatures and elevated PM2.5 concentrations, reinforcing the combined role of heat and air pollution in driving climate-related health risks.']], ['markdown', ['# Conclusion']], ['markdown', ['## Key Findings\\n', '\\n', '*   Correlations: Strong links between temperature and vector disease risk (0.65) and heat-related admissions (0.42) heat wave days and extreme weather are nearly collinear (0.96).\\n', '*   Protective factors: Healthcare access and food security reduce climate-health risks.\\n', '*   High-risk clusters: Overlap of high temperatures and PM2.5, especially in Indonesia, Kenya, and Bangladesh (chronic risk), and Philippines, Italy, Vietnam, France, and China (acute events).\\n', '*   Trend: Rising proportion of high-risk weeks from 2020â€“2024.\\n', '\\n', '## Model Performance\\n', '\\n', '*   Heat admissions (Regression): Random Forest best (RÂ² = 0.82); XGBoost close (RÂ² = 0.80) with faster training. Top predictors: heat wave days, temperature, precipitation, GDP, healthcare access.\\n', '*   Health risk (Classification): Random Forest (76.6% accuracy, AUC = 0.80, specificity >92%). Key drivers: heat waves, temperature, PM2.5, extreme weather, air quality.\\n', '\\n', '## Recommendations\\n', '\\n', '*   Policy: Target heat-health action plans in Philippines, Italy, Indonesia, Kenya, Bangladesh and improve air quality and strengthen healthcare/food systems.\\n', '*   Monitoring: Deploy early warning systems for areas with combined heat + PM2.5 exposure; tailor responses to acute vs. chronic vulnerability.\\n', '*   Research: Use higher-frequency data, study climate-socioeconomic interactions, and validate models with real-world outcomes.']], ['markdown', []]]\n"
     ]
    }
   ],
   "source": [
    "print(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b161f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "author = \"Ridhwan\"\n",
    "date = \"December 9th, 2025\"\n",
    "\n",
    "yaml_config = f\"\"\"---\n",
    "title: |\n",
    "  <center>WQD7004 Group 5</center>  \n",
    "  <center>Group Project</center>\n",
    "  <br />\n",
    "author: |\n",
    "  <p style=\"float: right\">{author}</p>\n",
    "  <br />\n",
    "date: |\n",
    "  <p style=\"float: right\">{date}</p>\n",
    "  <br />\n",
    "output:\n",
    "  html_document:\n",
    "    highlight: textmate\n",
    "    theme: flatly\n",
    "    number_sections: yes\n",
    "    toc: yes\n",
    "    toc_float:\n",
    "      collapse: yes\n",
    "      smooth_scroll: yes\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "file_text = yaml_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d288bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_type, source in cells:\n",
    "    if cell_type == \"markdown\" and source:\n",
    "        file_text += \"\".join(source) + \"\\n\\n\"\n",
    "    elif cell_type == \"code\" and source:\n",
    "        code_block = (\n",
    "f\"\"\"```{{{programming_language}}}\n",
    "{''.join(source)}\n",
    "```\\n\\n\"\"\"\n",
    "        )\n",
    "        \n",
    "        file_text += code_block + \"\\n\\n\"\n",
    "        \n",
    "        \n",
    "with open(\"converted_notebook.Rmd\", \"w\", encoding = \"UTF-8\") as file:\n",
    "    file.write(file_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
